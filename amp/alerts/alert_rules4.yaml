groups:
- name: k8s.rules4
  rules:
      - alert: PrometheusOperatorWatchErrors
        expr: (sum by(controller, namespace) (rate(prometheus_operator_watch_operations_failed_total{job="kube-prometheus-stack-operator",namespace="monitoring"}[10m])) / sum by(controller, namespace) (rate(prometheus_operator_watch_operations_total{job="kube-prometheus-stack-operator",namespace="monitoring"}[10m]))) > 0.4
        for: 15m
        labels:
          severity: warning
        annotations:
          description: Errors while performing watch operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorwatcherrors
          summary: () Errors while performing watch operations in controller.
      - alert: PrometheusOperatorSyncFailed
        expr: min_over_time(prometheus_operator_syncs{job="kube-prometheus-stack-operator",namespace="monitoring",status="failed"}[5m]) > 0
        for: 10m
        labels:
          severity: warning
        annotations:
          description: Controller {{ $labels.controller }} in {{ $labels.namespace }} namespace fails to reconcile {{ $value }} objects.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorsyncfailed
          summary: () Last controller reconciliation failed
      - alert: PrometheusOperatorReconcileErrors
        expr: (sum by(controller, namespace) (rate(prometheus_operator_reconcile_errors_total{job="kube-prometheus-stack-operator",namespace="monitoring"}[5m]))) / (sum by(controller, namespace) (rate(prometheus_operator_reconcile_operations_total{job="kube-prometheus-stack-operator",namespace="monitoring"}[5m]))) > 0.1
        for: 10m
        labels:
          severity: warning
        annotations:
          description: .{{ $value | humanizePercentage }} of reconciling operations failed for {{ $labels.controller }} controller in {{ $labels.namespace }} namespace.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorreconcileerrors
          summary: () Errors while reconciling controller.
      - alert: PrometheusOperatorNodeLookupErrors
        expr: rate(prometheus_operator_node_address_lookup_errors_total{job="kube-prometheus-stack-operator",namespace="monitoring"}[5m]) > 0.1
        for: 10m
        labels:
          severity: warning
        annotations:
          description: Errors while reconciling Prometheus in {{ $labels.namespace }} Namespace.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatornodelookuperrors
          summary: () Errors while reconciling Prometheus.
      - alert: PrometheusOperatorNotReady
        expr: min by(namespace, controller) (max_over_time(prometheus_operator_ready{job="kube-prometheus-stack-operator",namespace="monitoring"}[5m]) == 0)
        for: 5m
        labels:
          severity: warning
        annotations:
          description: Prometheus operator in {{ $labels.namespace }} namespace isn't ready to reconcile {{ $labels.controller }} resources.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatornotready
          summary: () Prometheus operator not ready
      - alert: PrometheusOperatorRejectedResources
        expr: min_over_time(prometheus_operator_managed_resources{job="kube-prometheus-stack-operator",namespace="monitoring",state="rejected"}[5m]) > 0
        for: 5m
        labels:
          severity: warning
        annotations:
          description: Prometheus operator in {{ $labels.namespace }} namespace rejected {{ printf "%0.0f" $value }} {{ $labels.controller }}/{{ $labels.resource }} resources.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorrejectedresources
          summary: () Resources rejected by Prometheus operator
      - alert: KubeContainerWaiting
        expr: sum by(namespace, pod, container) (kube_pod_container_status_waiting_reason{job="kube-state-metrics",namespace=~".*"}) > 0
        for: 1h
        labels:
          severity: warning
        annotations:
          description: pod/{{ $labels.pod }} in namespace {{ $labels.namespace }} on container {{ $labels.container}} has been in waiting state for longer than 1 hour.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubecontainerwaiting
          summary: () Pod container waiting longer than 1 hour
      - alert: KubeDeploymentReplicasMismatch
        expr: (kube_deployment_spec_replicas{job="kube-state-metrics",namespace=~".*"} > kube_deployment_status_replicas_available{job="kube-state-metrics",namespace=~".*"}) and (changes(kube_deployment_status_replicas_updated{job="kube-state-metrics",namespace=~".*"}[10m]) == 0)
        for: 15m
        labels:
          severity: warning
        annotations:
          description: Deployment {{ $labels.namespace }}/{{ $labels.deployment }} has not matched the expected number of replicas for longer than 15 minutes.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubedeploymentreplicasmismatch
          summary: () Deployment has not matched the expected number of replicas.
      - alert: KubePodNotReady
        expr: sum by(namespace, pod) (max by(namespace, pod) (kube_pod_status_phase{job="kube-state-metrics",namespace=~".*",phase=~"Pending|Unknown"}) * on(namespace, pod) group_left(owner_kind) topk by(namespace, pod) (1, max by(namespace, pod, owner_kind) (kube_pod_owner{owner_kind!="Job"}))) > 0
        for: 15m
        labels:
          severity: warning
        annotations:
          description: Pod {{ $labels.namespace }}/{{ $labels.pod }} has been in a non-ready state for longer than 15 minutes.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/kubernetes/kubepodnotready
          summary: () Pod has been in a non-ready state for more than 15 minutes.
        #- alert: PrometheusAlertmanagerE2eDeadManSwitch
        #expr: vector(1)
        #for: 0m
        #labels:
        #  severity: critical
        #annotations:
        #  summary: () Prometheus AlertManager E2E dead man switch (instance {{ $labels.instance }})
        #  description: "Prometheus DeadManSwitch is an always-firing alert. It's used as an end-to-end test of Prometheus through the Alertmanager.\n  VALUE = {{ $value }}\n  LABELS = {{ $labels }}"
      - alert: AlertmanagerFailedReload
        annotations:
          description: Configuration has failed to load for {{ $labels.namespace }}/{{
            $labels.pod}}.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerfailedreload
          summary: () Reloading an Alertmanager configuration has failed.
        expr: |-
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
          max_over_time(alertmanager_config_last_reload_successful{job="kube-prometheus-stack-alertmanager",namespace="istio-system"}[5m]) == 0
        for: 10m
        labels:
          severity: critical
      - alert: AlertmanagerMembersInconsistent
        annotations:
          description: Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} has only
            found {{ $value }} members of the {{$labels.job}} cluster.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagermembersinconsistent
          summary: () A member of an Alertmanager cluster has not found all other cluster
            members.
        expr: |-
          # Without max_over_time, failed scrapes could create false negatives, see
          # https://www.robustperception.io/alerting-on-gauges-in-prometheus-2-0 for details.
            max_over_time(alertmanager_cluster_members{job="kube-prometheus-stack-alertmanager",namespace="istio-system"}[5m])
          < on (namespace,service) group_left
            count by (namespace,service) (max_over_time(alertmanager_cluster_members{job="kube-prometheus-stack-alertmanager",namespace="istio-system"}[5m]))
        for: 15m
        labels:
          severity: critical
      - alert: AlertmanagerFailedToSendAlerts
        annotations:
          description: Alertmanager {{ $labels.namespace }}/{{ $labels.pod}} failed to
            send {{ $value | humanizePercentage }} of notifications to {{ $labels.integration
            }}.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerfailedtosendalerts
          summary: () An Alertmanager instance failed to send notifications.
        expr: |-
          (
            rate(alertmanager_notifications_failed_total{job="kube-prometheus-stack-alertmanager",namespace="istio-system"}[5m])
          /
            rate(alertmanager_notifications_total{job="kube-prometheus-stack-alertmanager",namespace="istio-system"}[5m])
          )
          > 0.01
        for: 5m
        labels:
          severity: warning
      - alert: AlertmanagerClusterFailedToSendAlerts
        annotations:
          description: The minimum notification failure rate to {{ $labels.integration
            }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage
            }}.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts
          summary: () All Alertmanager instances in a cluster failed to send notifications
            to a critical integration.
        expr: |-
          min by (namespace,service, integration) (
            rate(alertmanager_notifications_failed_total{job="kube-prometheus-stack-alertmanager",namespace="istio-system", integration=~`.*`}[5m])
          /
            rate(alertmanager_notifications_total{job="kube-prometheus-stack-alertmanager",namespace="istio-system", integration=~`.*`}[5m])
          )
          > 0.01
        for: 5m
        labels:
          severity: critical
      - alert: AlertmanagerClusterFailedToSendAlerts
        annotations:
          description: The minimum notification failure rate to {{ $labels.integration
            }} sent from any instance in the {{$labels.job}} cluster is {{ $value | humanizePercentage
            }}.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterfailedtosendalerts
          summary: () All Alertmanager instances in a cluster failed to send notifications
            to a non-critical integration.
        expr: |-
          min by (namespace,service, integration) (
            rate(alertmanager_notifications_failed_total{job="kube-prometheus-stack-alertmanager",namespace="istio-system", integration!~`.*`}[5m])
          /
            rate(alertmanager_notifications_total{job="kube-prometheus-stack-alertmanager",namespace="istio-system", integration!~`.*`}[5m])
          )
          > 0.01
        for: 5m
        labels:
          severity: warning
      - alert: AlertmanagerConfigInconsistent
        annotations:
          description: Alertmanager instances within the {{$labels.job}} cluster have
            different configurations.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerconfiginconsistent
          summary: () Alertmanager instances within the same cluster have different configurations.
        expr: |-
          count by (namespace,service) (
            count_values by (namespace,service) ("config_hash", alertmanager_config_hash{job="kube-prometheus-stack-alertmanager",namespace="istio-system"})
          )
          != 1
        for: 20m
        labels:
          severity: critical
      - alert: AlertmanagerClusterDown
        annotations:
          description: '{{ $value | humanizePercentage }} of Alertmanager instances within
            the {{$labels.job}} cluster have been up for less than half of the last 5m.'
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclusterdown
          summary: () Half or more of the Alertmanager instances within the same cluster
            are down.
        expr: |-
          (
            count by (namespace,service) (
              avg_over_time(up{job="kube-prometheus-stack-alertmanager",namespace="istio-system"}[5m]) < 0.5
            )
          /
            count by (namespace,service) (
              up{job="kube-prometheus-stack-alertmanager",namespace="istio-system"}
            )
          )
          >= 0.5
        for: 5m
        labels:
          severity: critical
      - alert: AlertmanagerClusterCrashlooping
        annotations:
          description: '{{ $value | humanizePercentage }} of Alertmanager instances within
            the {{$labels.job}} cluster have restarted at least 5 times in the last 10m.'
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/alertmanager/alertmanagerclustercrashlooping
          summary: () Half or more of the Alertmanager instances within the same cluster
            are crashlooping.
        expr: |-
          (
            count by (namespace,service) (
              changes(process_start_time_seconds{job="kube-prometheus-stack-alertmanager",namespace="istio-system"}[10m]) > 4
            )
          /
            count by (namespace,service) (
              up{job="kube-prometheus-stack-alertmanager",namespace="istio-system"}
            )
          )
          >= 0.5
        for: 5m
        labels:
          severity: critical
