groups:
- name: k8s.rules3
  rules:
      - alert: NodeNetworkInterfaceFlapping
        expr: changes(node_network_up{device!~"veth.+",job="node-exporter"}[2m]) > 2
        for: 2m
        labels:
          severity: warning
        annotations:
          description: Network interface "{{ $labels.device }}" changing its up status often on node-exporter {{ $labels.namespace }}/{{ $labels.pod }}
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/general/nodenetworkinterfaceflapping
          summary: () Network interface is often changing its status
      - alert: PrometheusBadConfig
        expr: max_over_time(prometheus_config_last_reload_successful{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) == 0
        for: 10m
        labels:
          severity: critical
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to reload its configuration.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusbadconfig
          summary: () Failed Prometheus configuration reload.
      - alert: PrometheusNotificationQueueRunningFull
        expr: (predict_linear(prometheus_notifications_queue_length{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m], 60 * 30) > min_over_time(prometheus_notifications_queue_capacity{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]))
        for: 15m
        labels:
          severity: warning
        annotations:
          description: Alert notification queue of Prometheus {{$labels.namespace}}/{{$labels.pod}} is running full.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotificationqueuerunningfull
          summary: () Prometheus alert notification queue predicted to run full in less than 30m.
      - alert: PrometheusErrorSendingAlertsToSomeAlertmanagers
        expr: (rate(prometheus_notifications_errors_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) / rate(prometheus_notifications_sent_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m])) * 100 > 1
        for: 15m
        labels:
          severity: warning
        annotations:
          description: .{{ printf "%.1f" $value }}% errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to Alertmanager {{$labels.alertmanager}}.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuserrorsendingalertstosomealertmanagers
          summary: () Prometheus has encountered more than 1% errors sending alerts to a specific Alertmanager.
      - alert: PrometheusNotConnectedToAlertmanagers
        expr: max_over_time(prometheus_notifications_alertmanagers_discovered{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) < 1
        for: 10m
        labels:
          severity: warning
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not connected to any Alertmanagers.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotconnectedtoalertmanagers
          summary: () Prometheus is not connected to any Alertmanagers.
      - alert: PrometheusTSDBReloadsFailing
        expr: increase(prometheus_tsdb_reloads_failures_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[3h]) > 0
        for: 4h
        labels:
          severity: warning
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} reload failures over the last 3h.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustsdbreloadsfailing
          summary: () Prometheus has issues reloading blocks from disk.
      - alert: PrometheusTSDBCompactionsFailing
        expr: increase(prometheus_tsdb_compactions_failed_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[3h]) > 0
        for: 4h
        labels:
          severity: warning
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has detected {{$value | humanize}} compaction failures over the last 3h.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustsdbcompactionsfailing
          summary: () Prometheus has issues compacting blocks.
      - alert: PrometheusNotIngestingSamples
        expr: (rate(prometheus_tsdb_head_samples_appended_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) <= 0 and (sum without(scrape_job) (prometheus_target_metadata_cache_entries{job="kube-prometheus-stack-prometheus",namespace="monitoring"}) > 0 or sum without(rule_group) (prometheus_rule_group_rules{job="kube-prometheus-stack-prometheus",namespace="monitoring"}) > 0))
        for: 10m
        labels:
          severity: warning
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is not ingesting samples.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusnotingestingsamples
          summary: () Prometheus is not ingesting samples.
      - alert: PrometheusDuplicateTimestamps
        expr: rate(prometheus_target_scrapes_sample_duplicate_timestamp_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) > 0
        for: 10m
        labels:
          severity: warning
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf "%.4g" $value  }} samples/s with different values but duplicated timestamp.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusduplicatetimestamps
          summary: () Prometheus is dropping samples with duplicate timestamps.
      - alert: PrometheusOutOfOrderTimestamps
        expr: rate(prometheus_target_scrapes_sample_out_of_order_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) > 0
        for: 10m
        labels:
          severity: warning
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} is dropping {{ printf "%.4g" $value  }} samples/s with timestamps arriving out of order.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusoutofordertimestamps
          summary: () Prometheus drops samples with out-of-order timestamps.
      - alert: PrometheusRemoteStorageFailures
        expr: ((rate(prometheus_remote_storage_failed_samples_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m])) / ((rate(prometheus_remote_storage_failed_samples_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) or rate(prometheus_remote_storage_samples_failed_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m])) + (rate(prometheus_remote_storage_succeeded_samples_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) or rate(prometheus_remote_storage_samples_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m])))) * 100 > 1
        for: 15m
        labels:
          severity: critical
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} failed to send {{ printf "%.1f" $value }}% of the samples to {{ $labels.remote_name}}:{{ $labels.url }}
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotestoragefailures
          summary: () Prometheus fails to send samples to remote storage.
      - alert: PrometheusRemoteWriteBehind
        expr: (max_over_time(prometheus_remote_storage_highest_timestamp_in_seconds{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) - ignoring(remote_name, url) group_right() max_over_time(prometheus_remote_storage_queue_highest_sent_timestamp_seconds{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m])) > 120
        for: 15m
        labels:
          severity: critical
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write is {{ printf "%.1f" $value }}s behind for {{ $labels.remote_name}}:{{ $labels.url }}.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotewritebehind
          summary: () Prometheus remote write is behind.
      - alert: PrometheusRemoteWriteDesiredShards
        expr: (max_over_time(prometheus_remote_storage_shards_desired{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) > max_over_time(prometheus_remote_storage_shards_max{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]))
        for: 15m
        labels:
          severity: warning
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} remote write desired shards calculation wants to run {{ $value }} shards for queue {{ $labels.remote_name}}:{{ $labels.url }}, which is more than the max of {{ printf `prometheus_remote_storage_shards_max{instance="%s",job="kube-prometheus-stack-prometheus",namespace="monitoring"}` $labels.instance | query | first | value }}.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusremotewritedesiredshards
          summary: () Prometheus remote write desired shards calculation wants to run more than configured max shards.
      - alert: PrometheusRuleFailures
        expr: increase(prometheus_rule_evaluation_failures_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) > 0
        for: 15m
        labels:
          severity: critical
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has failed to evaluate {{ printf "%.0f" $value }} rules in the last 5m.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusrulefailures
          summary: () Prometheus is failing rule evaluations.
      - alert: PrometheusMissingRuleEvaluations
        expr: increase(prometheus_rule_group_iterations_missed_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) > 0
        for: 15m
        labels:
          severity: warning
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has missed {{ printf "%.0f" $value }} rule group evaluations in the last 5m.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheusmissingruleevaluations
          summary: () Prometheus is missing rule evaluations due to slow rule group evaluation.
      - alert: PrometheusTargetLimitHit
        expr: increase(prometheus_target_scrape_pool_exceeded_target_limit_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) > 0
        for: 15m
        labels:
          severity: warning
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf "%.0f" $value }} targets because the number of targets exceeded the configured target_limit.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustargetlimithit
          summary: () Prometheus has dropped targets because some scrape configs have exceeded the targets limit.
      - alert: PrometheusLabelLimitHit
        expr: increase(prometheus_target_scrape_pool_exceeded_label_limits_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) > 0
        for: 15m
        labels:
          severity: warning
        annotations:
          description: Prometheus {{$labels.namespace}}/{{$labels.pod}} has dropped {{ printf "%.0f" $value }} targets because some samples exceeded the configured label_limit, label_name_length_limit or label_value_length_limit.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuslabellimithit
          summary: () Prometheus has dropped targets because some scrape configs have exceeded the labels limit.
      - alert: PrometheusTargetSyncFailure
        expr: increase(prometheus_target_sync_failed_total{job="kube-prometheus-stack-prometheus",namespace="monitoring"}[30m]) > 0
        for: 5m
        labels:
          severity: critical
        annotations:
          description: .{{ printf "%.0f" $value }} targets in Prometheus {{$labels.namespace}}/{{$labels.pod}} have failed to sync because invalid configuration was supplied.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheustargetsyncfailure
          summary: () Prometheus has failed to sync targets.
      - alert: PrometheusErrorSendingAlertsToAnyAlertmanager
        expr: min without(alertmanager) (rate(prometheus_notifications_errors_total{alertmanager!~"",job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m]) / rate(prometheus_notifications_sent_total{alertmanager!~"",job="kube-prometheus-stack-prometheus",namespace="monitoring"}[5m])) * 100 > 3
        for: 15m
        labels:
          severity: critical
        annotations:
          description: .{{ printf "%.1f" $value }}% minimum errors while sending alerts from Prometheus {{$labels.namespace}}/{{$labels.pod}} to any Alertmanager.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus/prometheuserrorsendingalertstoanyalertmanager
          summary: () Prometheus encounters more than 3% errors sending alerts to any Alertmanager.
      - alert: PrometheusOperatorListErrors
        expr: (sum by(controller, namespace) (rate(prometheus_operator_list_operations_failed_total{job="kube-prometheus-stack-operator",namespace="monitoring"}[10m])) / sum by(controller, namespace) (rate(prometheus_operator_list_operations_total{job="kube-prometheus-stack-operator",namespace="monitoring"}[10m]))) > 0.4
        for: 15m
        labels:
          severity: warning
        annotations:
          description: Errors while performing List operations in controller {{$labels.controller}} in {{$labels.namespace}} namespace.
          runbook_url: https://runbooks.prometheus-operator.dev/runbooks/prometheus-operator/prometheusoperatorlisterrors
          summary: () Errors while performing list operations in controller.
